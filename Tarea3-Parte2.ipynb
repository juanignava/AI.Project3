{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instituto Tecnológico de Costa Rica (ITCR)\n",
    "### Escuela de Computación\n",
    "### Curso: Inteligencia Artificial\n",
    " \n",
    "### Tercera tarea programada 2022-I\n",
    "\n",
    "### Parte 2 - ejercicio 2\n",
    "\n",
    "\n",
    "Estudiantes: Juan Ignacio Navarro Navarro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e4edb306b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "def max_values(x):\n",
    "    # Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    # Parámetros: \n",
    "    #    x: vector con los datos. \n",
    "    # Salida: \n",
    "    #    out: valor \n",
    "    #    inds: índice\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "\n",
    "# Preparación de los datos \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    # Retorna un tensor con los indices del diccionario para cada palabras en una oración.\n",
    "    # Parámetros:\n",
    "    #   seq: oración\n",
    "    #   to_ix: diccionario de palabras.\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'hotel', 'expensive', 'parking', 'got', 'good', 'deal', 'stay', 'hotel', 'anniversary,', 'arrived', 'late', 'evening', 'took', 'advice', 'previous', 'reviews', 'did', 'valet', 'parking,', 'check', 'quick', 'easy,', 'little', 'disappointed', 'non-existent', 'view', 'room', 'room', 'clean', 'nice', 'size,', 'bed', 'comfortable', 'woke', 'stiff', 'neck', 'high', 'pillows,', 'not', 'soundproof', 'like', 'heard', 'music', 'room', 'night', 'morning', 'loud', 'bangs', 'doors', 'opening', 'closing', 'hear', 'people', 'talking', 'hallway,', 'maybe', 'just', 'noisy', 'neighbors,', 'aveda', 'bath', 'products', 'nice,', 'did', 'not', 'goldfish', 'stay', 'nice', 'touch', 'taken', 'advantage', 'staying', 'longer,', 'location', 'great', 'walking', 'distance', 'shopping,', 'overall', 'nice', 'experience', 'having', 'pay', '40', 'parking', 'night,']\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  1,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 26, 27,  0, 28, 29, 30, 31, 32,\n",
      "        33, 34, 35, 36, 37, 38, 39, 40, 26, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 16, 36, 60,  7,  0, 61, 62, 63,\n",
      "        64, 65, 66, 67, 68, 69, 70, 71,  0, 72, 73, 74, 75,  3, 76])\n"
     ]
    }
   ],
   "source": [
    "# Loading data to csv\n",
    "df = pd.read_csv(r'tripadvisor_hotel_reviews.csv')\n",
    "# Lowercase columns\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Diccionario de palabras con su calificación\n",
    "\n",
    "training_data = []\n",
    "for i in df.index:\n",
    "    elem_tuple = df[\"review\"][i].split(), int(df[\"rating\"][i])\n",
    "    training_data.append(elem_tuple)\n",
    "\n",
    "# Diccionario las palabras\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
    "\n",
    "\n",
    "#Ejemplo de procesamiento de una oración\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(training_data[0][0])                          \n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    # Incialización del modelo\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "\n",
    "        # Primero se pasa la entrada a través de una capa Embedding. \n",
    "        # Esta capa construye una representación de los tokens de \n",
    "        # un texto donde las palabras que tienen el mismo significado \n",
    "        # tienen una representación similar.\n",
    "        \n",
    "        # Esta capa captura mejor el contexto y son espacialmente \n",
    "        # más eficientes que las representaciones vectoriales (one-hot vector).\n",
    "        # En Pytorch, se usa el módulo nn.Embedding para crear esta capa, \n",
    "        # que toma el tamaño del vocabulario y la longitud deseada del vector \n",
    "        # de palabras como entrada. \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # El LSTM toma word_embeddings como entrada y genera los estados ocultos\n",
    "        # con dimensionalidad hidden_dim.  \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # La capa lineal mapea el espacio de estado oculto \n",
    "        # al espacio de clases\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pasada hacia adelante de la red. \n",
    "        # Parámetros:\n",
    "        #    sentence: la oración a procesar\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "\n",
    "        # Se utiliza softmax para devolver la probabilidad de cada etiqueta\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "# Se definen pequeños, para ver cómo cambian los pesos durante el entrenamiento.\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Instancia del modelo\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Generalmente utilizada en problemas de clasificacion con C clases.\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'hotel', 'expensive', 'parking', 'got', 'good', 'deal', 'stay', 'hotel', 'anniversary,', 'arrived', 'late', 'evening', 'took', 'advice', 'previous', 'reviews', 'did', 'valet', 'parking,', 'check', 'quick', 'easy,', 'little', 'disappointed', 'non-existent', 'view', 'room', 'room', 'clean', 'nice', 'size,', 'bed', 'comfortable', 'woke', 'stiff', 'neck', 'high', 'pillows,', 'not', 'soundproof', 'like', 'heard', 'music', 'room', 'night', 'morning', 'loud', 'bangs', 'doors', 'opening', 'closing', 'hear', 'people', 'talking', 'hallway,', 'maybe', 'just', 'noisy', 'neighbors,', 'aveda', 'bath', 'products', 'nice,', 'did', 'not', 'goldfish', 'stay', 'nice', 'touch', 'taken', 'advantage', 'staying', 'longer,', 'location', 'great', 'walking', 'distance', 'shopping,', 'overall', 'nice', 'experience', 'having', 'pay', '40', 'parking', 'night,']\n",
      "tensor([[-2.0096, -1.8258, -1.9736, -1.5528, -1.9723, -1.5365],\n",
      "        [-2.0170, -1.8243, -2.0222, -1.5407, -1.9466, -1.5313],\n",
      "        [-2.0195, -1.8057, -2.0031, -1.6149, -1.9733, -1.4692],\n",
      "        [-2.0890, -1.7998, -2.0350, -1.6633, -1.9081, -1.4175],\n",
      "        [-2.0783, -1.8364, -1.9687, -1.7017, -1.9107, -1.4045],\n",
      "        [-2.1332, -1.8345, -2.0177, -1.5721, -1.9339, -1.4409],\n",
      "        [-2.1348, -1.8373, -2.0196, -1.6141, -1.8757, -1.4378],\n",
      "        [-2.1487, -1.8188, -2.0097, -1.5609, -1.9697, -1.4369],\n",
      "        [-2.0847, -1.8220, -2.0317, -1.6125, -1.8610, -1.4786],\n",
      "        [-2.0829, -1.8180, -2.0192, -1.5577, -1.9042, -1.5108],\n",
      "        [-2.0550, -1.7782, -1.9955, -1.7273, -1.9719, -1.3860],\n",
      "        [-2.0400, -1.8037, -1.9577, -1.8145, -1.9451, -1.3540],\n",
      "        [-2.1068, -1.8092, -2.0235, -1.5845, -1.9515, -1.4469],\n",
      "        [-2.0566, -1.8307, -1.9899, -1.6248, -1.8754, -1.4922],\n",
      "        [-2.0875, -1.8100, -2.0280, -1.6223, -1.9075, -1.4488],\n",
      "        [-2.0650, -1.8215, -2.0240, -1.5560, -1.9140, -1.5105],\n",
      "        [-2.0440, -1.7795, -2.0302, -1.5693, -1.9234, -1.5317],\n",
      "        [-2.0909, -1.8182, -1.9919, -1.5793, -1.9557, -1.4693],\n",
      "        [-2.1118, -1.8264, -2.0309, -1.5617, -1.8868, -1.4897],\n",
      "        [-2.0070, -1.8131, -1.9896, -1.5534, -1.9111, -1.5786],\n",
      "        [-2.0755, -1.8019, -1.9888, -1.6546, -1.9624, -1.4231],\n",
      "        [-2.0885, -1.7320, -2.0088, -1.7650, -2.0170, -1.3440],\n",
      "        [-2.1258, -1.7812, -2.0120, -1.6816, -1.9308, -1.3968],\n",
      "        [-2.1277, -1.8338, -1.9967, -1.7064, -1.8868, -1.3774],\n",
      "        [-2.1398, -1.8147, -1.9900, -1.5727, -1.9929, -1.4315],\n",
      "        [-2.1718, -1.8294, -2.0456, -1.5267, -1.9327, -1.4515],\n",
      "        [-2.1522, -1.7864, -2.0304, -1.5706, -1.9736, -1.4353],\n",
      "        [-2.0508, -1.7273, -1.9891, -1.7170, -2.0605, -1.3872],\n",
      "        [-2.0315, -1.7096, -1.9919, -1.8166, -2.0149, -1.3642],\n",
      "        [-2.0692, -1.7457, -2.0327, -1.7012, -1.9256, -1.4273],\n",
      "        [-2.0137, -1.7914, -1.9879, -1.6582, -1.8843, -1.5124],\n",
      "        [-2.0779, -1.8454, -1.9804, -1.6073, -1.8883, -1.4825],\n",
      "        [-2.0998, -1.8000, -2.0405, -1.5783, -1.9187, -1.4735],\n",
      "        [-2.1047, -1.8263, -2.0220, -1.5191, -1.9418, -1.5034],\n",
      "        [-2.0565, -1.7631, -2.0299, -1.6685, -1.9348, -1.4428],\n",
      "        [-2.1346, -1.8425, -2.0256, -1.5226, -1.9704, -1.4532],\n",
      "        [-2.0032, -1.7951, -2.0199, -1.5676, -1.8962, -1.5720],\n",
      "        [-2.0618, -1.7992, -2.0327, -1.5378, -1.9415, -1.5232],\n",
      "        [-2.0504, -1.7614, -2.0307, -1.5796, -2.0222, -1.4703],\n",
      "        [-2.0893, -1.8155, -2.0331, -1.5422, -1.9367, -1.4941],\n",
      "        [-2.1178, -1.8315, -2.0124, -1.6248, -1.9357, -1.4081],\n",
      "        [-2.1108, -1.8053, -2.0200, -1.7122, -1.9065, -1.3756],\n",
      "        [-2.1995, -1.7965, -2.0006, -1.7284, -1.9281, -1.3280],\n",
      "        [-2.1973, -1.8423, -2.0119, -1.6447, -1.8799, -1.3821],\n",
      "        [-2.0767, -1.7611, -1.9755, -1.7684, -2.0252, -1.3412],\n",
      "        [-2.1113, -1.7703, -2.0413, -1.7479, -1.8868, -1.3741],\n",
      "        [-2.0962, -1.7668, -2.0714, -1.7069, -1.8703, -1.4078],\n",
      "        [-2.1457, -1.8025, -2.0475, -1.6154, -1.9480, -1.3956],\n",
      "        [-2.0367, -1.7461, -2.0505, -1.6208, -1.9479, -1.4868],\n",
      "        [-2.0644, -1.8001, -2.0255, -1.5998, -1.9587, -1.4571],\n",
      "        [-2.1256, -1.8299, -2.0320, -1.5410, -1.9861, -1.4366],\n",
      "        [-2.1217, -1.8219, -2.0593, -1.5530, -1.9407, -1.4452],\n",
      "        [-2.1206, -1.8577, -2.0685, -1.5771, -1.9137, -1.4125],\n",
      "        [-2.1258, -1.8124, -2.0492, -1.5436, -1.9447, -1.4614],\n",
      "        [-2.0713, -1.7805, -1.9991, -1.6620, -1.9912, -1.4119],\n",
      "        [-2.0277, -1.7422, -1.9826, -1.7498, -2.0266, -1.3866],\n",
      "        [-2.0986, -1.8203, -1.9970, -1.6333, -1.9965, -1.3921],\n",
      "        [-2.0956, -1.8453, -2.0143, -1.5435, -1.9768, -1.4548],\n",
      "        [-2.1068, -1.8196, -1.9891, -1.6532, -1.9275, -1.4167],\n",
      "        [-2.0554, -1.7708, -2.0384, -1.6308, -1.8877, -1.4945],\n",
      "        [-2.0659, -1.7924, -2.0347, -1.5888, -1.9102, -1.4970],\n",
      "        [-2.1051, -1.7670, -2.0417, -1.5761, -1.9817, -1.4572],\n",
      "        [-2.1236, -1.8129, -2.0130, -1.6082, -1.8988, -1.4539],\n",
      "        [-2.1432, -1.8144, -1.9956, -1.6025, -1.9574, -1.4220],\n",
      "        [-2.1544, -1.8365, -1.9850, -1.5707, -1.9520, -1.4380],\n",
      "        [-2.1542, -1.8633, -2.0130, -1.5162, -1.8943, -1.4904],\n",
      "        [-2.0531, -1.8348, -2.0326, -1.7404, -1.9043, -1.3601],\n",
      "        [-2.1045, -1.8042, -2.0016, -1.5941, -1.9729, -1.4431],\n",
      "        [-2.0408, -1.8405, -1.9785, -1.5891, -1.8848, -1.5275],\n",
      "        [-2.0425, -1.8039, -1.9850, -1.6854, -1.9715, -1.4121],\n",
      "        [-2.1038, -1.8366, -2.0024, -1.5172, -1.9783, -1.4872],\n",
      "        [-2.1045, -1.8188, -2.0408, -1.5975, -1.8907, -1.4585],\n",
      "        [-2.1360, -1.8376, -2.0161, -1.5391, -1.9518, -1.4571],\n",
      "        [-2.0832, -1.7758, -2.0099, -1.7048, -1.9650, -1.3857],\n",
      "        [-2.0836, -1.8471, -1.9815, -1.6014, -1.9372, -1.4514],\n",
      "        [-2.0603, -1.8345, -2.0084, -1.5648, -1.9061, -1.5101],\n",
      "        [-2.0704, -1.8330, -2.0153, -1.5910, -1.8645, -1.5052],\n",
      "        [-2.0478, -1.7998, -2.0492, -1.5987, -1.8770, -1.5069],\n",
      "        [-2.0663, -1.8630, -2.0317, -1.5598, -1.9399, -1.4561],\n",
      "        [-2.0690, -1.7746, -2.0383, -1.6001, -1.9652, -1.4615],\n",
      "        [-2.0091, -1.8076, -1.9919, -1.6029, -1.9120, -1.5311],\n",
      "        [-2.0893, -1.8102, -2.0173, -1.6880, -1.9010, -1.4053],\n",
      "        [-2.1176, -1.8270, -2.0212, -1.6387, -1.8908, -1.4226],\n",
      "        [-2.1570, -1.8044, -2.0206, -1.6417, -1.9241, -1.3963],\n",
      "        [-2.1283, -1.8120, -1.9967, -1.5888, -1.9878, -1.4243],\n",
      "        [-2.1528, -1.7964, -2.0181, -1.6801, -1.9037, -1.3877],\n",
      "        [-2.0303, -1.7471, -2.0646, -1.8647, -1.9270, -1.3204]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juan navarro\\Documents\\Implementaciones - AI\\AI.Project3\\Tarea3-Parte2.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000012?line=20'>21</a>\u001b[0m \u001b[39m# Paso 2. Se preparan las entradas, es decir, se convierten a\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000012?line=21'>22</a>\u001b[0m \u001b[39m# tensores de índices de palabras.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000012?line=22'>23</a>\u001b[0m sentence_in \u001b[39m=\u001b[39m prepare_sequence(sentence, word_to_ix)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000012?line=23'>24</a>\u001b[0m targets \u001b[39m=\u001b[39m prepare_sequence(tags, tag_to_ix)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000012?line=25'>26</a>\u001b[0m \u001b[39m# Paso 3. Se genera la predicción (forward pass).\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000012?line=26'>27</a>\u001b[0m tag_scores \u001b[39m=\u001b[39m model(sentence_in)\n",
      "\u001b[1;32mc:\\Users\\juan navarro\\Documents\\Implementaciones - AI\\AI.Project3\\Tarea3-Parte2.ipynb Cell 3'\u001b[0m in \u001b[0;36mprepare_sequence\u001b[1;34m(seq, to_ix)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000009?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_sequence\u001b[39m(seq, to_ix):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000009?line=14'>15</a>\u001b[0m     \u001b[39m# Retorna un tensor con los indices del diccionario para cada palabras en una oración.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000009?line=15'>16</a>\u001b[0m     \u001b[39m# Parámetros:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000009?line=16'>17</a>\u001b[0m     \u001b[39m#   seq: oración\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000009?line=17'>18</a>\u001b[0m     \u001b[39m#   to_ix: diccionario de palabras.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000009?line=18'>19</a>\u001b[0m     idxs \u001b[39m=\u001b[39m [to_ix[w] \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m seq]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juan%20navarro/Documents/Implementaciones%20-%20AI/AI.Project3/Tarea3-Parte2.ipynb#ch0000009?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(idxs, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "# El elemento i, j de la salida es la puntuación entre la etiqueta j para la palabra i.\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "    print(training_data[0][0])\n",
    "    \n",
    "    # Clasificación    \n",
    "    print(tag_scores)\n",
    "\n",
    "# Épocas de entrenamiento\n",
    "for epoch in range(500):  \n",
    "    for sentence, tags in training_data:\n",
    "        ## Paso 1. Pytorch acumula los gradientes.\n",
    "        # Es necesario limpiarlos\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Paso 2. Se preparan las entradas, es decir, se convierten a\n",
    "        # tensores de índices de palabras.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Paso 3. Se genera la predicción (forward pass).\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \n",
    "        # parámetros por medio del optimizador.\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Despligue de la puntuación luego del entrenamiento\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "   \n",
    "    print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "    # Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "    # La primera oración tiene 4 palabras \"El perro come manzana\"\n",
    "    # por eso el tensor de salida tiene 4 elementos. \n",
    "    # Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "    # posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "    # la posición del valor máximo\n",
    "    print(tag_scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e868495d694aed0b609784ddbe0f4170fa3a0dc0623318ab45c72f62595d740c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
